{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949e04df",
   "metadata": {},
   "source": [
    "# Getting the data\n",
    "TODO make work with day 2\n",
    "\n",
    "### development notes\n",
    "The important code from this notebook has been exported to `data_collection_pipeline.py`,\n",
    "and is being updated throughout the project. Currently, changes from the 'wrangling' phase have been implemented.\n",
    "This document was updated to reflect these changes, but isn't 'perfect'\n",
    "This document is meant as a development notebook, and any future use of the pipe line should use the python file.\n",
    "Using this to test changes to the pipeline could be useful still, so I'll do my best to keep this updated going forward.\n",
    "\n",
    "\n",
    "\n",
    "Every year, bushiroad hosts a large circut of tournaments all over the world. The goal of this pipeline is to take the information published about each circute (the event information, and the tournament report), and gather all the data in one place for analysis.\n",
    "\n",
    "The circut starts with an annoucement, and a webpage going up with all the information on the events.\n",
    "We need to use a webscraper that:\n",
    "* Takes this new URL as an input\n",
    "* For each event in the circut, creates a list of the:\n",
    "    - region,\n",
    "    - location,\n",
    "    - and date,\n",
    "    - then outputs this information to a 'super list' for storage\n",
    "\n",
    "This list will not be enough on it's own to get us started, since the event reports have extenstions that don't perfectly match to a patern. In the future, a webscraper could be prepared to help with the next step, but it's easy enogh to manage for now.\n",
    "The event report website 'hub' will take an extension to go to the 'branch' for a particular event.\n",
    "By taking these extensions, and adding them to each corresponding 'sub list', our next level of the pipeline can get the information about each event\n",
    "\n",
    "Bushiroad uploads a list of all the players who participated in each of their higher level events, with each player's:\n",
    "* handle\n",
    "* placement\n",
    "* \\# of wins\n",
    "* nation\n",
    "* grade 3 in r.d.\n",
    "* decklog id\n",
    "\n",
    "This information is uploaded to a URL, with each page's HTML containing links to decklog, so we will need to:\n",
    "* create a dataframe out of the information on the main page\n",
    "* follow those decklog links to obtain more detailed data\n",
    "* (in the future, look up individual card names for even more detailed data)\n",
    "* store information about just this event in one place for easy isolation and redundancy\n",
    "* add data about the event we got from the previous stage (location, region, date)\n",
    "* store it all for future use in a central database for easy comparison between events\n",
    "\n",
    "The data from decklists doesn't fit too fell into a traditional database of rows and columns, but all the information can fit neatly into dictionaries, or JSON.\n",
    "Each object we save to the database should contain 2 'levels' of data:\n",
    "* the information from the original webpage at the base level at the base level\n",
    "* a dictionary contaiing information about the deck list, which is nested dictionary, containing a dict each for the\n",
    "    * main,\n",
    "    * ride,\n",
    "    * and g decks\n",
    "\n",
    "##### In the future, cards may have `id`'s instead of names, so that more detailed card information can be stored in a central location\n",
    "\n",
    "Each 'layer' will be populated by a different pipleine.\n",
    "A \"zero-th\" 'preperation' pipeline was created to gather information about each event before getting data from the event reports. (Region, Location, and Date)\n",
    "Pipeline 1 will get data from the central page, placing it into the first dictionary, as well as add data from the preperation pipeline, and format the data appropriately.\n",
    "and pipelie 2 will take the dekcklogs frome pipeline 1, obtain data from each link, using it to fill the second dictionary\n",
    "A third pipeline will save this data to the database.\n",
    "\n",
    "##### NOTE\n",
    "Due to the structure of the main table we're scraping having special scripts for the ride deck g3's of the top 3 players, when using the second pipeline, the ride deck g3's name needs to be extracted, and as a key value pair to the first dicionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7785b5e3",
   "metadata": {},
   "source": [
    "# Part 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b18e3ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import used in this notebook\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "# For webscrapping pages after loading\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.expected_conditions\\\n",
    "    import presence_of_element_located as present\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f92aa",
   "metadata": {},
   "source": [
    "The first approach of determining which evens to do was a little bit of manual clicking, but gathering the dates and regions felt like a bit too much manual effort. So, we've whipped up a script to make a base list to use, containing some of the public info on each event, such as city, region, and date.\n",
    "We'll still need to manually verify and enter some of the bushiroad event report links to get the data into the format we want.\n",
    "\n",
    "Maybe next year, we can write a scraper to scrape all the links on the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c22a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# NEW EVENT INFO SCRAOING TOOL (PRE_PREP PIPELINE)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "def scrape_event_info():\n",
    "    \"\"\"\n",
    "    This function is to prepare our event list for our main function.\n",
    "    The output of this function is expected to be manually edited\n",
    "    before being fed into main;\n",
    "    ) Names need to be changed\n",
    "    _ URLs need to be manually obtained from the event report site\n",
    "\n",
    "    When re-using this function, start by changing the URL,\n",
    "    and go from there. Good luck :P\n",
    "    \"\"\"\n",
    "\n",
    "    URL = 'https://en.bushiroad.com/events/bcs2526/schedule/'\n",
    "    request = requests.get(URL)\n",
    "    soup=Soup(request.text, 'html.parser')\n",
    "\n",
    "    region_map = {\n",
    "        'regional-na': 'NA',\n",
    "        'regional-eu': 'EU',\n",
    "        'regional-asia': 'AO'\n",
    "    }\n",
    "    final_event_list = []\n",
    "    # Credt: Geminin Pro\n",
    "    # 3. Iterate through the defined regions\n",
    "    for region_id, continent_code in region_map.items():\n",
    "        # Find the main container for this specific continent\n",
    "        region_container = soup.find('div', id=region_id)\n",
    "\n",
    "        if not region_container:\n",
    "            print(f\"-> Info: Container for ID '{region_id}' ({continent_code}) not found in this HTML snippet. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # print(f\"-> Processing continent: {continent_code}\")\n",
    "\n",
    "        # 4. Find individual event cards within this region container.\n",
    "        # In your HTML, every event seems to be wrapped in a div with class=\"event-card\"\n",
    "        event_cards = region_container.find_all('div', class_='event-card')\n",
    "\n",
    "        for card in event_cards:\n",
    "            # Extract Location Name\n",
    "            # It looks like <h5 class=\"mb-0\">City Name (Country)</h5>\n",
    "            location_tag = card.find('h5', class_='mb-0')\n",
    "            if location_tag:\n",
    "                # .get_text(strip=True) removes surrounding whitespace and HTML tags\n",
    "                location_name = location_tag.get_text(strip=True)\n",
    "            else:\n",
    "                # Fallback: try getting it from the data-city attribute if the h5 fails\n",
    "                location_name = card.get('data-city', 'Unknown Location')\n",
    "\n",
    "            # Extract Date\n",
    "            # It looks like <p class=\"sm-txt schedule-date\">Date Range</p>\n",
    "            date_tag = card.find('p', class_='schedule-date')\n",
    "            if date_tag:\n",
    "                date_text = date_tag.get_text(strip=True)\n",
    "            else:\n",
    "                date_text = \"Unknown Date\"\n",
    "\n",
    "            # Create the sublist [Location, Continent, Date]\n",
    "            event_info = [location_name, continent_code, date_text]\n",
    "            final_event_list.append(event_info)\n",
    "\n",
    "    # print(\"\\nExtraction complete. Here is your list:\")\n",
    "    # print(\"-\" * 30)\n",
    "    # # Pretty print the final list\n",
    "    # for item in final_event_list:\n",
    "        # print(item)\n",
    "\n",
    "    # If you want to use this list later in the script, it is stored in `final_event_list`\n",
    "    # print(\"\\nRaw list output:\")\n",
    "    # print(final_event_list)# Credt: Geminin Pro\n",
    "    # 3. Iterate through the defined regions\n",
    "    for region_id, continent_code in region_map.items():\n",
    "        # Find the main container for this specific continent\n",
    "        region_container = soup.find('div', id=region_id)\n",
    "\n",
    "        if not region_container:\n",
    "            print(f\"-> Info: Container for ID '{region_id}' ({continent_code}) not found in this HTML snippet. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # print(f\"-> Processing continent: {continent_code}\")\n",
    "\n",
    "        # 4. Find individual event cards within this region container.\n",
    "        # In your HTML, every event seems to be wrapped in a div with class=\"event-card\"\n",
    "        event_cards = region_container.find_all('div', class_='event-card')\n",
    "\n",
    "        for card in event_cards:\n",
    "            # Extract Location Name\n",
    "            # It looks like <h5 class=\"mb-0\">City Name (Country)</h5>\n",
    "            location_tag = card.find('h5', class_='mb-0')\n",
    "            if location_tag:\n",
    "                # .get_text(strip=True) removes surrounding whitespace and HTML tags\n",
    "                location_name = location_tag.get_text(strip=True)\n",
    "            else:\n",
    "                # Fallback: try getting it from the data-city attribute if the h5 fails\n",
    "                location_name = card.get('data-city', 'Unknown Location')\n",
    "\n",
    "            # Extract Date\n",
    "            # It looks like <p class=\"sm-txt schedule-date\">Date Range</p>\n",
    "            date_tag = card.find('p', class_='schedule-date')\n",
    "            if date_tag:\n",
    "                date_text = date_tag.get_text(strip=True)\n",
    "            else:\n",
    "                date_text = \"Unknown Date\"\n",
    "\n",
    "            # Create the sublist [Location, Continent, Date]\n",
    "            event_info = [location_name, continent_code, date_text]\n",
    "            final_event_list.append(event_info)\n",
    "\n",
    "    # print(\"\\nExtraction complete. Here is your list:\")\n",
    "    # print(\"-\" * 30)\n",
    "    # # Pretty print the final list\n",
    "    # for item in final_event_list:\n",
    "        # print(item)\n",
    "\n",
    "    # If you want to use this list later in the script, it is stored in `final_event_list`\n",
    "    # print(\"\\nRaw list output:\")\n",
    "    # print(final_event_list)\n",
    "\n",
    "    return final_event_list\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# events = scrape_event_info()\n",
    "# events\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e7424",
   "metadata": {},
   "source": [
    "\n",
    "Below is the copied and edited list.\n",
    "\n",
    "The events list scraped is going to need some manual fixing up, for now. \n",
    "\n",
    "TODO's represent events who's data hasn't been collected yet. Mostly future dates.\n",
    "\n",
    "Maybe for next year, we can add a webscraper to automate this part as well :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf412bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_event_list=\\\n",
    "[\n",
    "    # ['illinois', 'Rosemont', 'NA', 'October 4, 2025'],\n",
    "    # ['mexico', 'Mexico', 'NA', 'November 8, 2025'],\n",
    "    # ['bcs2526-houston-tx', 'TX', 'NA', 'November 22, 2025'],\n",
    "    # ['bcs2526-california', 'LA', 'NA', 'December 6, 2025'],\n",
    "    # ['vancouver', 'BC', 'NA', 'January 10, 2026'],\n",
    "    # ['argentina', 'Argentina', 'NA', 'January 17, 2026'],\n",
    "    # ['duluth', 'Duluth', 'NA', 'January 17, 2026'],\n",
    "    # ['puerto-rico', 'Puerto Rico', 'NA', 'January 24, 2026'],\n",
    "    \n",
    "    # ['TODO', 'Toronto', 'NA', 'February 14, 2026'],\n",
    "    # ['TODO', 'Costa Rica', 'NA', 'February 21, 2026'],\n",
    "    # ['TODO', 'Philadelphia', 'NA', 'March 21, 2026'],\n",
    "\n",
    "    \n",
    "    # ['modling-austria', 'Austria', 'EU', 'November 15, 2025'],\n",
    "    # ['italy', 'Italy', 'EU', 'December 13, 2025'],\n",
    "    # ['spain', 'Spain', 'EU', 'January 3, 2026'],\n",
    "    # ['france', 'France', 'EU', 'February 7, 2026'],\n",
    "    \n",
    "    # ['TODO', 'Germany', 'EU', 'February 21, 2026'],\n",
    "    # ['TODO', 'Greece', 'EU', 'March 7, 2026'],\n",
    "    # ['TODO', 'United Kingdom', 'EU', 'March 21, 2026'],\n",
    "\n",
    "    \n",
    "    # ['ho-chi-minh-city-vietnam', 'Vietnam', 'AO', 'November 2, 2025'],\n",
    "    # ['surabaya-indonesia', 'Indonesia', 'AO', 'November 16, 2025'],\n",
    "    # ['bcs2526-malaysia', 'Malaysia', 'AO', 'December 6, 2025'],\n",
    "    # ['manila', 'Philippines', 'AO', 'January 17, 2026'],\n",
    "    # ['singapore', 'Singapore', 'AO', 'January 24, 2026'],\n",
    "    \n",
    "    # ['TODO', 'Melbourne, Australia', 'AO', 'February 28, 2026'],\n",
    "    # ['TODO', 'Sydney, Australia', 'AO', 'March 28, 2026'],\n",
    "    # ['TODO', 'Indonesia', 'AO', 'March 28, 2026']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d545d8f2",
   "metadata": {},
   "source": [
    "## **Part** 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef5db5a",
   "metadata": {},
   "source": [
    "# **INPUT** This script requires an input sring, 'EVENT' in order to function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a920d",
   "metadata": {},
   "source": [
    "### Obtain HTML for event, and extract to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5377b4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "644dd31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values(row):\n",
    "    \"\"\"This procederal function deals with the fact that the first 3 rows of\n",
    "    the table have a slightly different structure than the rest.\n",
    "\n",
    "    While I could write two functions, one for the first 3 rows, \n",
    "    and one for the rest, this just felt more simple and natural to write,\n",
    "    allthough it's a bit tough to read.\n",
    "\n",
    "    Our i counter being manual allows us to skip over the missing row\n",
    "    for our top 3 players, while still keeping the incremental couting logic.\n",
    "\n",
    "    Since the first 3 rows are a different size, we need to adjust the amount of values,\n",
    "    so our data frame is happy. It ended up seeming simplest to just add `None` as we pass through\n",
    "    \"\"\"\n",
    "    values=list()\n",
    "    for i, item in enumerate(row):\n",
    "        if i == 1: # This double conditional is true when the first 3 row's second element\n",
    "            if item.div: # is a division, instead of text. we need to treat it different\n",
    "                values.append(\"Champion\") # we could extract the name, but it would be very complex\n",
    "                values.append(None) # Add a none placeholder for our missing value\n",
    "                continue # this continue statement seperates this special behaviour from the simple case\n",
    "\n",
    "        values.append(row[i].text.strip())\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221a6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        rank              name                                      boss  wins  \\\n",
       "146MF     1          Champion                                      None    11   \n",
       "6EZB7     2          Champion                                      None    11   \n",
       "42N7X     3          Champion                                      None    10   \n",
       "1AQ5W     4               Ken           Fated One of Taboo, Zorga Nadir    10   \n",
       "6B3RK     5               Kay  Fated One of Unparalleled, Varga Dragres     8   \n",
       "...     ...               ...                                       ...   ...   \n",
       "3NLY5   485       gilalmighty              Omniscience Regalia, Minerva     0   \n",
       "4HGK7   486  Guardian Paladin              Soul Awakening Guard, Leuhan     0   \n",
       "2LQPN   487       Nessiechomp    Demon Stealth Dragon, Shiranui ‘Oboro’     0   \n",
       "4DG85   488           Metelx8          Super Dimensional Robo, Daiyusha     0   \n",
       "5WRG9   489        RidazD3135                         Dragonic Overlord     0   \n",
       "\n",
       "                nation decklog  deck location region              date  \n",
       "146MF      Brandt Gate   146MF  None       LA     NA  December 6, 2025  \n",
       "6EZB7      Dark States   6EZB7  None       LA     NA  December 6, 2025  \n",
       "42N7X        Stoicheia   42N7X  None       LA     NA  December 6, 2025  \n",
       "1AQ5W        Stoicheia   1AQ5W  None       LA     NA  December 6, 2025  \n",
       "6B3RK    Dragon Empire   6B3RK  None       LA     NA  December 6, 2025  \n",
       "...                ...     ...   ...      ...    ...               ...  \n",
       "3NLY5  Keter Sanctuary   3NLY5  None       LA     NA  December 6, 2025  \n",
       "4HGK7  Keter Sanctuary   4HGK7  None       LA     NA  December 6, 2025  \n",
       "2LQPN    Dragon Empire   2LQPN  None       LA     NA  December 6, 2025  \n",
       "4DG85      Brandt Gate   4DG85  None       LA     NA  December 6, 2025  \n",
       "5WRG9    Dragon Empire   5WRG9  None       LA     NA  December 6, 2025  \n",
       "\n",
       "[489 rows x 10 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Full example for testing\n",
    "# FULL_URL = \"https://en.cf-vanguard.com/event/bcs2526/bcs2526-california/\"\n",
    "\n",
    "# Use this to substitute for the input this function will recieve\n",
    "# EVENT = 'bcs2526-california'\n",
    "\n",
    "# After creation, we've started tracking more data. \n",
    "# WOP: Updateing the function to work with this new input:\n",
    "# Event info schema\n",
    "# event_info =[\n",
    "#     url_extension, # <- this was the original input\n",
    "#     converted_name,\n",
    "#     region,\n",
    "#     date\n",
    "# ]\n",
    "\n",
    "TEST_EVENT = [\n",
    "    'bcs2526-california',\n",
    "    'LA',\n",
    "    'NA',\n",
    "    'December 6, 2025'\n",
    "]\n",
    "\n",
    "# These are for indexing into the event info list object\n",
    "URL_EXT = 0\n",
    "NAME = 1\n",
    "REGION = 2\n",
    "DATE = 3\n",
    "\n",
    "BASE_URL = \"https://en.cf-vanguard.com/event/bcs2526/\"\n",
    "\n",
    "base_url=BASE_URL\n",
    "event_info=TEST_EVENT\n",
    "\n",
    "\n",
    "# Part 1: Get basic info and decklog from event page ~~~~~~~~~~~~~~~~~~~~~~\n",
    "url = f'{base_url}{event_info[URL_EXT]}'\n",
    "soup = Soup(requests.get(url).text, features='html.parser')\n",
    "rows = soup.table.find_all('tr')\n",
    "# Remove header row, so all rows have td, and not th\n",
    "rows.pop(0)\n",
    "\n",
    "dataDict = dict()\n",
    "for row in rows:\n",
    "    # Decklog as the key\n",
    "    key = row['data-deck-id']\n",
    "    values = get_values(row.find_all('td'))\n",
    "    values.append(key)\n",
    "    values.append(None) # deck info place holder\n",
    "\n",
    "    dataDict[key] = values\n",
    "\n",
    "df = pd.DataFrame(dataDict).transpose()\n",
    "df = df.set_axis([\n",
    "                    'rank',#\n",
    "                    'name',\n",
    "                    'boss',\n",
    "                    'wins',#\n",
    "                    'nation',\n",
    "                    'decklog',\n",
    "                    'deck'\n",
    "                ], \n",
    "                axis=1)\n",
    "\n",
    "# Part 1.5 - Wrangle the data ~~~~~~~~~~~~~`\n",
    "# the rank and wins column need to be converted to ints,\n",
    "# and now we have some new attributes that need to be encoded.`\n",
    "df['rank'] = df['rank'].str[:-2].astype(int)\n",
    "df['wins'] = df['wins'].astype(int)\n",
    "df['location'] = event_info[NAME]\n",
    "df['region'] = event_info[REGION]\n",
    "df['date'] = event_info[DATE]\n",
    "\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ef13215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>wins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>489.000000</td>\n",
       "      <td>489.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>245.000000</td>\n",
       "      <td>2.822086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>141.306405</td>\n",
       "      <td>1.953098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>123.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>245.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>367.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>489.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             rank        wins\n",
       "count  489.000000  489.000000\n",
       "mean   245.000000    2.822086\n",
       "std    141.306405    1.953098\n",
       "min      1.000000    0.000000\n",
       "25%    123.000000    1.000000\n",
       "50%    245.000000    3.000000\n",
       "75%    367.000000    4.000000\n",
       "max    489.000000   11.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10729744",
   "metadata": {},
   "source": [
    "## Part 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fba3af",
   "metadata": {},
   "source": [
    "### Obtain HTML for each deck's decklog, and add to df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d9e9be",
   "metadata": {},
   "source": [
    "Now, we need to settup a function to run on each decklog that we have in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea6235e",
   "metadata": {},
   "source": [
    "Not every decklog will have a G zone, and will thus contain a different number of 'row' divisions.\n",
    "As far as standard decks go, there will be 7 rows if there is no g zone, and up one more to 8 with one.\n",
    "The use of `None` for the G deck will help us skip over it without creating any errors.\n",
    "These magic numbers for indexing below are a bit cringe, so I hope to come back and fix this later.\n",
    "\n",
    "**NOTE: THIS WILL NEED UPDATING FOR PREMIUM DECKS DUE TO MAGIC NUMBER HACK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b96ab9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decklogToDict(soup):\n",
    "    \"\"\"\n",
    "    TODO make work with premium\n",
    "    \n",
    "    Given the BeautifulSoup for a decklogs...\n",
    "    \"\"\"\n",
    "    # Initialize the decks we return\n",
    "    boss = None\n",
    "    rideDeckDict = dict()\n",
    "    mainDeckDict = dict()\n",
    "    gDeckDict = dict()\n",
    "    deckDict = {\n",
    "        'RideDeck': rideDeckDict,\n",
    "        'MainDeck': mainDeckDict,\n",
    "        'GDeck': gDeckDict\n",
    "    }\n",
    "\n",
    "    # obtain data from soup\n",
    "    rows =  soup.find_all('div', 'row')\n",
    "    rideDeck = rows[5].find_all('div', 'card-controller')\n",
    "    mainDeck = rows[6].find_all('div', 'card-controller')\n",
    "    if len(rows) == 8:\n",
    "        gDeck = rows[7].find_all('div', 'card-controller')\n",
    "    else:\n",
    "        gDeck = None\n",
    "\n",
    "    # pair soup data with dict\n",
    "    decks = {\n",
    "        \"RideDeck\": rideDeck, \n",
    "        \"MainDeck\": mainDeck,\n",
    "        \"GDeck\": gDeck\n",
    "    }\n",
    "\n",
    "    # extract data from the soup into the dictionary\n",
    "    for deck in deckDict:\n",
    "        if not decks[deck]: continue\n",
    "        for card in decks[deck]:\n",
    "            spans = card.find_all('span')\n",
    "            namespan= str(spans[0])\n",
    "            index1 = namespan.find(' : ') + 3 #trim whitespace\n",
    "            index2 = namespan.find('\"></span>')\n",
    "            \n",
    "            card_name = namespan[index1:index2]\n",
    "            quant = int(spans[1].text)\n",
    "        \n",
    "            if card_name in deckDict[deck]:\n",
    "                deckDict[deck][card_name] += quant\n",
    "            else:\n",
    "                boss = boss or card_name\n",
    "                deckDict[deck][card_name] =  quant\n",
    "            \n",
    "    return deckDict, boss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff1bcdd",
   "metadata": {},
   "source": [
    "Above, we created the framework to hold all the information.\n",
    "Now, below, we will scrape all the information from the page into the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40f3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit Gemini for skeletron\n",
    "\n",
    "# Setup Chrome to run headless (without a visible window)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "# Initialize the browser\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options) \n",
    "waiter = WebDriverWait(driver=driver, timeout=20)\n",
    "\n",
    "# Go to each page\n",
    "BASE_URL = \"https://decklog-en.bushiroad.com/view/\" \n",
    "for i, row in df.iterrows():\n",
    "    # Most of the issues with getting deck logs just need a retry.\n",
    "    tries = 0\n",
    "    while tries < 3:\n",
    "        tries += 1\n",
    "        try:\n",
    "            if df.at[i, 'deck'] != None: continue # skip completed decks on re-run\n",
    "            code = df.at[i, 'decklog']\n",
    "            url = BASE_URL + str(code)\n",
    "            driver.get(url)\n",
    "            # Wait for JavaScript to execute (you can use smarter waits, but sleep is simple for testing)\n",
    "            waiter.until(\n",
    "                present((By.CLASS_NAME, \"card-controller\"))\n",
    "            )\n",
    "            html = driver.page_source\n",
    "            deckSoup = Soup(html)\n",
    "            deckDict, boss = decklogToDict(deckSoup)\n",
    "            df.at[i, 'deck'] = deckDict\n",
    "            #TODO MAKE WORK WITH PREMIUM DECKS\n",
    "            df.at[i, 'boss'] = boss\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# clean-up\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d3140",
   "metadata": {},
   "source": [
    "This bit ensures we finished getting each deck, in case we run into connection issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43666860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if None in df.deck: raise ValueError to preserve database authenticity when re-using this notebook\n",
    "todo = df.deck.isnull().sum()\n",
    "if todo != 0: raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76331d",
   "metadata": {},
   "source": [
    "I've encountered some errors with weird decklogs that aren't correct. [5.97E+06, 2.62E+02].\n",
    "\n",
    "I checked that these were issues on the Bushi site, and not the script, and those were the logs on the site.\n",
    "\n",
    "These logs are incorrect, and don't take to a valid deck log page. It's important we keep the script functional even when it recieves bad inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621face",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "### Upload the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32045040",
   "metadata": {},
   "source": [
    "TODO FIX SECURITY ISSUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# # Connect to MongoDB\n",
    "# username = 'sjmichael17_db_user'\n",
    "# password = 'rVtL43eBjseB5XkS' # plz don't hack me bro ;-;\n",
    "# cluster_address = 'bcsproto.peazuyx.mongodb.net/?appName=BCSproto'\n",
    "\n",
    "# client = MongoClient(f'mongodb+srv://{username}:{password}@{cluster_address}')\n",
    "\n",
    "# db = client['JSONproto']\n",
    "# collection = db[event_info[NAME]]\n",
    "\n",
    "# cleanDF = df.reset_index().rename(columns={'index':'_id'})\n",
    "# collection.insert_many(cleanDF.to_dict(orient='records'))\n",
    "\n",
    "# print(\"Done\")\n",
    "# client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
