{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b0bc996-689f-401d-96d6-40f64a36509b",
   "metadata": {},
   "source": [
    "# *Data centralization & Wrangling*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52678335",
   "metadata": {},
   "source": [
    "This file is a form of record keeping. It was used to develop a rework of the data collection pipeline, repaair data that had already been uploaded, and create some JSON files for testing. This is not acitvely used for development, and most of it's information has been moved into the Data Collection file, or to the main file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf0d4b-ff42-4e03-8713-85de5d5974d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location info schema\n",
    "# location_info =[\n",
    "#     cluster_name,\n",
    "#     converted_name,\n",
    "#     region,\n",
    "#     date\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f0bbd",
   "metadata": {},
   "source": [
    "After using this code, it should become antiquated.\n",
    "Ideally, all the changes we add to the existing data should be mirrored by an addition to the data pipeline to make sure that future data automatically will contain these changes. \n",
    "This seperation allows for the isolated deveelpment of these changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27df5fc-d21a-40f5-8aaa-87b2f4d83761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pandas import DataFrame as DF\n",
    "\n",
    "NAME = 0\n",
    "NEWNAME = 1\n",
    "REGION = 2\n",
    "DATE = 3\n",
    "\n",
    "def convert_bcs(location_info):\n",
    "    # plz dont hack me bro\n",
    "    username = \"mac_laptop\"\n",
    "    password = \"ThlXLTHVrBUdaGVZ\"\n",
    "    client = MongoClient(f'mongodb+srv://{username}:{password}@BCSproto.peazuyx.mongodb.net/')\n",
    "    # cluster = client['BCSproto']\n",
    "    db = client['JSONproto']\n",
    "    collection = db[location_info[NAME]]\n",
    "\n",
    "    collection.update_many(\n",
    "        {}, # no filter\n",
    "        [{\n",
    "            \"$set\": {\n",
    "                \"rank\": {\n",
    "                    \"$toInt\": {\n",
    "                        # remove the 2 letters in the string, so it can convert to number\n",
    "                        \"$substr\": [\"$rank\", 0, {\"$subtract\": [{\"$strLenCP\": \"$rank\"}, 2]}]\n",
    "                    }\n",
    "                },\n",
    "                \"wins\": {\"$toInt\": \"$wins\"},\n",
    "                \"decklog\": \"$_id\",\n",
    "                \"location\": location_info[NEWNAME],\n",
    "                \"region\": location_info[REGION],\n",
    "                \"date\": location_info[DATE]\n",
    "            }\n",
    "        }]\n",
    "    )\n",
    "\n",
    "    # dump collection to memory\n",
    "    df = DF(list(collection.find()))\n",
    "    # The ID needs to be removed to prevent conflicts with the main table\n",
    "    df.drop(columns=['_id'], inplace=True)\n",
    "\n",
    "    # Save as Json for testing \n",
    "    # from os import path\n",
    "    # file_path = path.join('.json', f'{location_info[NEWNAME]}.json')\n",
    "    # df.to_json(file_path, orient='records')\n",
    "\n",
    "    # add everything to the main table\n",
    "    main_table = client['main_table']\n",
    "    test = main_table['test']\n",
    "    test.insert_many(df.to_dict(orient='records'))\n",
    "\n",
    "\n",
    "    client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ae5277-10a7-4d33-b1ee-9fa52bd79ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc = \\\n",
    "#     ['puerto-rico', 'Puerto Rico', 'NA', 'January 24, 2026']\n",
    "# convert_bcs(loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1178cd70-2c54-4a40-8d20-0211bb3642db",
   "metadata": {},
   "source": [
    "Well, now we have this function, but while we're at it, we should be adding new data on the location and date of these events. It's a bit tough, but we can create a scraper to get all this information off the bushi website. The script that does this scraping should be saved for reuse on future event circuts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4188f7c3-4622-427f-bbe8-59f99c7d94a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import requests\n",
    "URL = 'https://en.bushiroad.com/events/bcs2526/schedule/'\n",
    "request = requests.get(URL)\n",
    "soup=Soup(request.text, 'html.parser')\n",
    "\n",
    "region_map = {\n",
    "    'regional-na': 'NA',\n",
    "    'regional-eu': 'EU',\n",
    "    'regional-asia': 'AO'\n",
    "}\n",
    "final_event_list = []\n",
    "\n",
    "# Credt: Geminin Pro\n",
    "# 3. Iterate through the defined regions\n",
    "for region_id, continent_code in region_map.items():\n",
    "    # Find the main container for this specific continent\n",
    "    region_container = soup.find('div', id=region_id)\n",
    "\n",
    "    if not region_container:\n",
    "        print(f\"-> Info: Container for ID '{region_id}' ({continent_code}) not found in this HTML snippet. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # print(f\"-> Processing continent: {continent_code}\")\n",
    "\n",
    "    # 4. Find individual event cards within this region container.\n",
    "    # In your HTML, every event seems to be wrapped in a div with class=\"event-card\"\n",
    "    event_cards = region_container.find_all('div', class_='event-card')\n",
    "\n",
    "    for card in event_cards:\n",
    "        # Extract Location Name\n",
    "        # It looks like <h5 class=\"mb-0\">City Name (Country)</h5>\n",
    "        location_tag = card.find('h5', class_='mb-0')\n",
    "        if location_tag:\n",
    "            # .get_text(strip=True) removes surrounding whitespace and HTML tags\n",
    "            location_name = location_tag.get_text(strip=True)\n",
    "        else:\n",
    "            # Fallback: try getting it from the data-city attribute if the h5 fails\n",
    "            location_name = card.get('data-city', 'Unknown Location')\n",
    "\n",
    "        # Extract Date\n",
    "        # It looks like <p class=\"sm-txt schedule-date\">Date Range</p>\n",
    "        date_tag = card.find('p', class_='schedule-date')\n",
    "        if date_tag:\n",
    "            date_text = date_tag.get_text(strip=True)\n",
    "        else:\n",
    "            date_text = \"Unknown Date\"\n",
    "\n",
    "        # Create the sublist [Location, Continent, Date]\n",
    "        event_info = [location_name, continent_code, date_text]\n",
    "        final_event_list.append(event_info)\n",
    "\n",
    "# print(\"\\nExtraction complete. Here is your list:\")\n",
    "# print(\"-\" * 30)\n",
    "# # Pretty print the final list\n",
    "# for item in final_event_list:\n",
    "    # print(item)\n",
    "\n",
    "# If you want to use this list later in the script, it is stored in `final_event_list`\n",
    "# print(\"\\nRaw list output:\")\n",
    "# print(final_event_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36871f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_event_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbc933",
   "metadata": {},
   "source": [
    "Now, I want to do a bit of manual data massage to ensure that the event names are saved the way I want for future analysis. \n",
    "For example, I want the Cali events to all have the same name every year, when they move to a neighboring suburb. IDK too much about the cities use for EU anually, so to be safe, we'll use the country name. They rarely if ever have 2 events in the same country. \n",
    "The only country other than the US to have 2 events is Australia. \n",
    "\n",
    "TODO : research history of australian events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead8c15a-6593-4f3e-89b6-7f12c08cfb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_event_list=\\\n",
    "[\n",
    "    # ['illinois', 'Rosemont', 'NA', 'October 4, 2025'],\n",
    "    # ['mexico', 'Mexico', 'NA', 'November 8, 2025'],\n",
    "    # ['bcs2526-houston-tx', 'TX', 'NA', 'November 22, 2025'],\n",
    "    # ['bcs2526-california', 'LA', 'NA', 'December 6, 2025'],\n",
    "    # ['vancouver', 'BC', 'NA', 'January 10, 2026'],\n",
    "    # ['argentina', 'Argentina', 'NA', 'January 17, 2026'],\n",
    "    # ['duluth', 'Duluth', 'NA', 'January 17, 2026'],\n",
    "    # ['puerto-rico', 'Puerto Rico', 'NA', 'January 24, 2026'],\n",
    "    \n",
    "    # ['TODO', 'Toronto', 'NA', 'February 14, 2026'],\n",
    "    # ['TODO', 'Costa Rica', 'NA', 'February 21, 2026'],\n",
    "    # ['TODO', 'Philadelphia', 'NA', 'March 21, 2026'],\n",
    "\n",
    "    \n",
    "    # ['modling-austria', 'Austria', 'EU', 'November 15, 2025'],\n",
    "    # ['italy', 'Italy', 'EU', 'December 13, 2025'],\n",
    "    # ['spain', 'Spain', 'EU', 'January 3, 2026'],\n",
    "    # ['france', 'France', 'EU', 'February 7, 2026'],\n",
    "    \n",
    "    # ['TODO', 'Germany', 'EU', 'February 21, 2026'],\n",
    "    # ['TODO', 'Greece', 'EU', 'March 7, 2026'],\n",
    "    # ['TODO', 'United Kingdom', 'EU', 'March 21, 2026'],\n",
    "\n",
    "    \n",
    "    # ['ho-chi-minh-city-vietnam', 'Vietnam', 'AO', 'November 2, 2025'],\n",
    "    # ['surabaya-indonesia', 'Indonesia', 'AO', 'November 16, 2025'],\n",
    "    # ['bcs2526-malaysia', 'Malaysia', 'AO', 'December 6, 2025'],\n",
    "    # ['manila', 'Philippines', 'AO', 'January 17, 2026'],\n",
    "    # ['singapore', 'Singapore', 'AO', 'January 24, 2026'],\n",
    "    \n",
    "    # ['TODO', 'Melbourne, Australia', 'AO', 'February 28, 2026'],\n",
    "    # ['TODO', 'Sydney, Australia', 'AO', 'March 28, 2026'],\n",
    "    # ['TODO', 'Indonesia', 'AO', 'March 28, 2026']\n",
    "]\n",
    "for location in final_event_list:\n",
    "    convert_bcs(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3b7f0-4aef-487e-80e8-997c4187752d",
   "metadata": {},
   "source": [
    "\n",
    "Everything not marked TODO is done, and in our central database now :)\n",
    "We might have to do something like this for the reaming elements, so we should take a break here, and see if we can't update our original Data collection process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9df53c",
   "metadata": {},
   "source": [
    "Okay, now that we've updated the `data_collection_pipeline.py` script with the preperation step of creating the table, and adding the changes we made to the data as part of the pipeline, this side shoot can be done, and we can get back to EDA! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
